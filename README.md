GWASkb
------

GWASkb is a machine-compiled knowledge base of associations between genetic mutations and human traits. 
* In our forthcoming paper, we describe our methodology for creating the database and analyze the results.
* In this repository, we walk through the code used to create the database.
* In our online portal at http://gwaskb.stanford.edu/, you can search the resulting database.

## Main Results

GWASkb contains associations in the form of tuples of (`genetic variant`, `trait`, `pvalue`). In our paper, we have selected and analyzed a set of associations that strike a good tradeoff between precision and recall.
These are found in: 

```
notebooks/results/associations.tsv
```

This is a tab-separated file with 5 columns: `pmid`, `rsid`, high-level phenotype, low-level phenotype, log p-value. If the latter is `-10000`, it means that we were not able to extract the p-value.

Our knowledge base also contains a large set of other data, which is documented in `results.md`.

## Structure

This repo is organized as follows:

```
.
├── README.md
├── annotations           # Manually annotated data
  └── not_in_gwasc.xlsx   # Manually annotated set of 100 relations extracted by GwasKB that were not in GWAS Catalog
├── data                  # Datasets from which the knowledge base was compiled 
  ├── associations        # Human-curated associations against which we compare
  ├── db                  # Scripts to download and create the input database of publications
  └── phenotypes          # Scripts to generate phenotype ontology used by the system
├── notebooks             # Jupyter notebooks that walk us through how the system was used to generate the results
  ├── bio-analysis        # Notebooks that reproduce the biological analysis performed in the paper
  ├── lfs.py              # A Python file containing all labeling functions used
  └── results             # The main set of results produced by the machine curation system
    ├── nb-output         # Intermediary output generated by each module (each notebook)
    └── metadata          # Metadata associated with extracted p-values
├── snorkel-tables        # Version of Snorkel used in the project
├── src                   # Source code of the components used on top of Snorkel
  ├── crawler             # Scripts used to generate a database of papers as well as to crawl human-curated DBs
  └── extractor           # Modules that extend Snorkel to extracting GWAS-specific from the publications
└── results.md            # File documenting the output of the system
```

In addition, the following files are important:

* `notebooks/results/nb-output`: folder containing the output of each system module
* `notebooks/util/phenotype.mapping.annotated.tsv`: manually annotated mapping between GWAS Central and GWASkb phenotypes
* `notebooks/util/phenotype.mapping.gwascat.annotated.tsv`: manually annotated mapping between GWAS Catalog and GWASkb phenotypes
* `notebooks/util/rels.discovered.annotated.txt`: random subset of 100 previously unreported relations with explanations for why they are correct or not.

## Requirements

GWASkb is intended to run on macOS and Unix (no GPU required).
It has been tested on macOS 10.14 (Mojave) and Ubuntu 16.04.

It requires python 2.7 and the following primary libraries:

* `lxml`, `ElementTree`
* `numpy`
* `sklearn`
* `sqlite`
* `snorkel`

These (and their dependencies) will be downloaded during Installation.

## Installation

### Step 1: Download source code
If you already have the source code, skip to Step 2.
If you are retrieving the source code from this repository, run the following commands:

```
git clone https://github.com/kuleshov/gwaskb.git
cd gwaskb;
git submodule init;
git submodule update;
```

### Step 2: Setup environment
We recommend using Anaconda to set up a virtual environment and working within that, 
but this is not strictly necessary, so long as python 2.7 is on your path.

[1] Check Python version
```
python --version
```
You should see Python 2.7.X. 
If not, you may need to create a virtual environment where Python 2.7 is used.

[2] Install dependencies
```
cd ./snorkel-tables
pip install --requirement python-package-requirement.txt
./run.sh            # Install treedlib and the Stanford CoreNLP tools
# This will also open a jupyter notebook which we will use for the demo in Step 4.

cd ..               # Return to root directory
source set_env.sh   # Add environment variables to your path
```

### Step 3: Download data
We extract mutation/phenotype relations from the open-access subset of PubMed.

In addition, we use hand-curated databases such as GWAS Catalog and GWAS Central for evaluation, and we use various ontologies (EFO, SNOMED, etc.) for phenotype extraction.

The first step is to download this data onto your machine and the easiest way to do that is to download a zipfile with the data that we used in the  notebooks:
```
https://drive.google.com/file/d/1DX17UCztwXtB3PxKLQd2waUBJdSdNJDc/view?usp=sharing
```

#### Generating the data manually

Alternatively, you may use our code to manually recreate this dataset.

This can be done in one step:
```
cd data/db
make
cd ../..
```
Or step-by-step using the instructions below.

```
cd data/db

# we will store part of the dataset in a sqlite databset
make init # this will initialize an empty database

# next, we load a database of known phenotypes that might occur in the literature
# this will load phenotypes from the EFO ontology as well as 
# various ontologies collected by the Hazy Research group
make phenotypes

# next, we download the contents of the hand-curated GWAS catalog database 
make gwas-catalog # loads into sqlite db (/tmp/gwas.sql by default); this takes a while

# now, let's download from pubmed all the open-access papers mentioned in the GWAS catalog
make dl-papers # downloads ~600 papers + their supplementary material!

# finally, we will use the GWAS central database for validation of the results
make gwas-central # this will only download the parts of GWAS central relevant to our papers
```

## Step 4: Information Extraction Demo

We demo our system in a series of Jupyter notebooks in the `notebooks` subfolder.

1. `phenotype-extraction.ipynb` identifies the phenotypes studied in each paper
2. `table-pval-extraction.ipynb` extracts mutation ids and their associated p-values
3. `table-phenotype-extraction.ipynb` extracts relations between mutations and a specific phenotype (out of the many that can be described in the paper)
4. `acronym-extraction.ipynb`: often, phenotypes are mentioned via acronyms, and we need a module to resolve those acronyms
5. `evaluation.ipynb`: here, we merge all the results and evaluate our accuracy

The result is a list of TSV files containing facts (e.g. mutation/disease relations) that we have extracted from the literature.  

## Feedback

Please send feedback to [Volodymyr Kuleshov](http://web.stanford.edu/~kuleshov/).

## Appendix
Database Design
1.	Overview
Analysing the initially provided database, we found that its format references the ICGC (International Cancer Genome Consortium). This research project was initiated in 2008 with the aim of accelerating advances in cancer treatment and diagnosis by comprehensively mapping the cancer genome, identifying the genetic variants that drive cancer, and opening up this database to researchers around the globe.The ICGC's database contains mutation data, gene expression data, clinical data, and genetic data[1].

2.	Required data and structure
Due to hardware and software limitations, this assignment is based on a small database created from an Excel-formatted data sheet provided by the university. The database fully contains the amount of information provided by the approximately 300,000 data items in the table, which can be classified as sample and mutation data in the ICGC database.
To realise the interactive functionality of the website, we have added several new patients with fictitious identifying information such as names and created a clinical database based on their basic information. In addition, according to the functionality required by the syllabus, we created a database containing information about oncologists.
To ensure that the information about patients and oncologists is authentic and reliable, we integrated the data directly into the Patient or Oncologist table. In our conception, the data should come from accredited healthcare or research institutions, shared through external databases, and identified by a unique patient ID or oncologist ID.
Following the principle of voluntariness, patients can give their consent to provide mutation data for this database either anonymously (only icgc_specimen_id) or with their real names (details such as name in addition to icgc_specimen_id). However, in order to maintain the security of the database, Oncologists with access and modification rights require more stringent authentication. This section will be discussed in the next paragraph as well as in the section on web filtering logic.
	The user database enables us to accurately identify the user based on the unique userID generated when registering the user based on the data already in the database. Therefore, when registering a user with the role of Oncologist, the unique oncologist ID needs to be verified from the existing database, and in extreme cases where the same person has different roles (e.g., when there is a certain oncologist who also has cancer), they should also have different user IDs to differentiate between different permissions. For patients who volunteer their real names in order to view information about their mutations, the registration of the user ID can be verified by the existing patient ID in the database.
Due to our lack of manpower, the function of Input new mutation has been deleted for the sake of fairness. However, in the future, if there is a need in this area, the database can still be updated through a series of query operations.
	
3.	Table design
This part was assigned to Xinyue because of our poor consideration during the pre-task assignment, but it is closely related to the database and directly or indirectly affects the shape of the database and the interaction between the front and back end, so we had a lot of close discussions and finally decided that it should be executed by me (Ningjia).

3.1 Normalisation, layout, design
For the initial provided mutation table (here called original), it needs to be Normalized according to the data structure, in order to facilitate the retrieval, I firstly generated a unique primary key ID for each record, and then, analyse the data in it, which can be roughly divided into several types of data.

	1）Cancer table
	The cancer_type in the table is a text type, which takes up a lot of memory, and these types will appear several times in the following table, it is better to extract them as a new table.
It contains 8 items of data from the original database and 2 items of data from the demo database, as well as 10 types that may appear in the future, for a total of 22 items of data.

	2）Mutation Table
	The table generates a unique Mutation ID (primary key), location (chromosome, start-end,...). . Due to the strong correlation of the subsequent data (often occurring at the same time when filtering) and for ease of comparison, no further simplification was performed. Although many of the same texts are present in TYPE, they are of fewer types, take up little storage resources, and are similarly retained.
It contains 297,431 entries from the original database and 14 entries from the demo database, for a total of 297,445 entries.

	3）Link table between icgc-specimen-id and Mutation
	This table corresponds all the icgc- specimen-id in Original to all the mutation IDs it contains, indicating the unique Mutation corresponding to each piece of data.This mapping table ensures that all the information in Original is retained, and significantly reduces the memory footprint, especially for duplicate data entries.
It contains 303028 entries from the Original database and 14 entries from the demo database, for a total of 303042 entries.

	4）Gene table
	This table generates a unique geneID as the primary key, corresponding to a unique gene. since there are many variants that are located in the current non-gene region, or for other reasons that do not affect the gene, they are shown as NULL values in the Original table. This optimisation is also important because it eliminates the presence of null values in the table (corresponding NULL values to geneID=1), facilitating data validation and future maintenance.
	This includes 36,378 entries from the original database and 2 entries from the demo database, for a total of 36,380 entries.
 
  5) Link table between geneID and Mutation
Since the same gene ID may correspond to multiple Mutation ID mutations, in order to simplify the Mutation table while ensuring that the information content of the data is not reduced, a correspondence table between gene ID and Mutation was created.
The table contains 186,459 data from the original database and 8 data from the demo database, totalling 186,467 data.

	6) Patient table
	Stores information about the patient. Includes 4 data for the demo database.

	7) Oncologist table
Stores information about the patient. Includes 3 data items from the demo database.

	8) User table
Stores information about users who have registered through the web page.

	9) Specimen and Patient Correspondence Table
	If there is a need to update the Mutation database in the future, this table can be queried. Includes 4 data from the demo database.

	10) Action table
Stores the activities performed by each user.

Reference
[1]The International Cancer Genome Consortium. International network of cancer genome projects. Nature 464, 993–998 (2010). https://doi.org/10.1038/nature08987


